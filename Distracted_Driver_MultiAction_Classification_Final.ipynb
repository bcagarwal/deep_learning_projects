{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b38t3QOn0XCl"
      },
      "outputs": [],
      "source": [
        "!wget -q -O distracted_driver.zip \"https://www.dropbox.com/s/0vyzjcqsdl6cqi2/state-farm-distracted-driver-detection.zip?dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/distracted_driver.zip -d distracted_driver_data"
      ],
      "metadata": {
        "id": "W23u9d1502ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/distracted_driver_data/imgs.zip -d distracted_driver_data/imgs"
      ],
      "metadata": {
        "id": "P4p5-llB24KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTur1i134Nly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FmkU4BW_6JpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7kprvMBdV1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Required Paths for Train and Val data set\n",
        "SOURCE_DIR = \"distracted_driver_data/imgs/train\"\n",
        "SPLIT_DIR_BASE_PATH = \"split_data\"\n",
        "TRAIN_DIR = os.path.join(SPLIT_DIR_BASE_PATH, \"train\")\n",
        "VAL_DIR = os.path.join(SPLIT_DIR_BASE_PATH, \"val\")"
      ],
      "metadata": {
        "id": "u-0pHOj871D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_images(dir) :\n",
        "  # Dictionary to store counts\n",
        "  class_counts = {}\n",
        "\n",
        "  # Iterate through each class folder\n",
        "  for class_name in sorted(os.listdir(dir)):\n",
        "      class_path = os.path.join(dir, class_name)\n",
        "      if os.path.isdir(class_path):\n",
        "          count = len([\n",
        "              file for file in os.listdir(class_path)\n",
        "              if file.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "          ])\n",
        "          class_counts[class_name] = count\n",
        "\n",
        "  # Print results\n",
        "  for class_name, count in class_counts.items():\n",
        "      print(f\"{class_name}: {count} images\")"
      ],
      "metadata": {
        "id": "6838YFCodXw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_images(SOURCE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owV3W8xedxO-",
        "outputId": "dbe92982-76d3-4f53-96d2-8104f77d0226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c0: 2489 images\n",
            "c1: 2267 images\n",
            "c2: 2317 images\n",
            "c3: 2346 images\n",
            "c4: 2326 images\n",
            "c5: 2312 images\n",
            "c6: 2325 images\n",
            "c7: 2002 images\n",
            "c8: 1911 images\n",
            "c9: 2129 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output folders\n",
        "for base in [TRAIN_DIR, VAL_DIR]:\n",
        "    for c in range(10):\n",
        "        os.makedirs(os.path.join(base, f\"c{c}\"), exist_ok=True)"
      ],
      "metadata": {
        "id": "mMxh17s28ldo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "H3lVjUTC8tHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label in os.listdir(SOURCE_DIR):\n",
        "    class_dir = os.path.join(SOURCE_DIR, label)\n",
        "    for img_file in os.listdir(class_dir):\n",
        "        image_paths.append(os.path.join(class_dir, img_file))\n",
        "        labels.append(label)"
      ],
      "metadata": {
        "id": "njTSqMWz9Gob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(image_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NzChL0P92__",
        "outputId": "b8fe6127-b7d9-4a2e-f192-feafd83827a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into train and val\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")"
      ],
      "metadata": {
        "id": "z9hZKUJJ9Nwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path, label in zip(train_paths, train_labels):\n",
        "    shutil.copy(path, os.path.join(TRAIN_DIR, label, os.path.basename(path)))"
      ],
      "metadata": {
        "id": "kOtMvV8_-GB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path, label in zip(val_paths, val_labels):\n",
        "    shutil.copy(path, os.path.join(VAL_DIR, label, os.path.basename(path)))"
      ],
      "metadata": {
        "id": "sEOtOYYU-GWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohE6XOcQAHOH",
        "outputId": "6b9aee25-a60f-4717-b327-c4466f904043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 17939, Validation samples: 4485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR, VAL_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o_PPowpAlgX",
        "outputId": "215cb248-8a7e-4b32-bf08-51c9e239cd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('split_data/train', 'split_data/val')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "8NZmtHSQDRg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "AgT7FIuMEyV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXHvuQaaE20Z",
        "outputId": "8c16637d-f871-45ac-bd42-e94abe16871c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "mcJjIEMbFEK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(base_model):\n",
        "  x = base_model.output\n",
        "  global_pool = GlobalAveragePooling2D(name='global_avg_pooling')(x)\n",
        "  bn1=BatchNormalization(name='bn1')(global_pool)\n",
        "  dropout1 = Dropout(0.3, name='dropout1')(bn1)\n",
        "\n",
        "  dense1 = Dense(128, activation='relu',name='dense1')(dropout1)\n",
        "  bn2=BatchNormalization(name='bn2')(dense1)\n",
        "  dropout2 = Dropout(0.4, name='dropout2')(bn2)\n",
        "\n",
        "  output = Dense(10, activation='softmax', name='output')(dropout2)\n",
        "\n",
        "  model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "yaFu7_1GGA-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "i3hNYMrnGIxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model(base_model)"
      ],
      "metadata": {
        "id": "AVjjf94VGLUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "o14LTnhPGNZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "id": "fkU8H94cGPLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "xpPGo47lGcNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_generator() :\n",
        "  generator = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest');\n",
        "  return generator"
      ],
      "metadata": {
        "id": "0zxm1oV5GekH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uzgjJ8a_JM7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_generator(d1,d2):\n",
        "\n",
        "  #train_datagen = create_data_generator()\n",
        "  #val_datagen = create_data_generator()\n",
        "\n",
        "  train_generator = create_data_generator().flow_from_directory(TRAIN_DIR,\n",
        "                                                    target_size=(d1,d2),\n",
        "                                                    batch_size=BATCH_SIZE, #16\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle=False\n",
        "                                                    )\n",
        "  val_generator = create_data_generator().flow_from_directory(VAL_DIR,\n",
        "                                                    target_size=(d1,d2),\n",
        "                                                    batch_size=BATCH_SIZE, #16\n",
        "                                                    class_mode='categorical')\n",
        "  return train_generator,val_generator"
      ],
      "metadata": {
        "id": "-KY2mYxdIAhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator,val_generator = create_generator(*IMG_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hacgjs8SIcyI",
        "outputId": "d7383d65-eb93-41f6-eac5-f819371781a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17939 images belonging to 10 classes.\n",
            "Found 4485 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = train_generator.n // train_generator.batch_size\n",
        "validation_steps = val_generator.n // val_generator.batch_size\n",
        "\n",
        "steps_per_epoch, validation_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o26AsZTFIjGY",
        "outputId": "e3e12b36-47db-4500-a621-cf3b0a1d9587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(560, 140)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator,\n",
        "                    # steps_per_epoch=steps_per_epoch,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_generator,\n",
        "                    # validation_steps=validation_steps\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw1AXla1J0qu",
        "outputId": "473f5633-7bb3-4d48-80f2-f4057941de94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 566ms/step - accuracy: 0.1031 - loss: 3.1904 - val_accuracy: 0.1043 - val_loss: 2.4046\n",
            "Epoch 2/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 554ms/step - accuracy: 0.0973 - loss: 2.8789 - val_accuracy: 0.0990 - val_loss: 2.3903\n",
            "Epoch 3/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 547ms/step - accuracy: 0.1027 - loss: 2.7156 - val_accuracy: 0.1175 - val_loss: 2.3430\n",
            "Epoch 4/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 542ms/step - accuracy: 0.1017 - loss: 2.6297 - val_accuracy: 0.0999 - val_loss: 2.3329\n",
            "Epoch 5/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 545ms/step - accuracy: 0.0994 - loss: 2.5799 - val_accuracy: 0.1043 - val_loss: 2.3205\n",
            "Epoch 6/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 544ms/step - accuracy: 0.1036 - loss: 2.5200 - val_accuracy: 0.0941 - val_loss: 2.3189\n",
            "Epoch 7/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 540ms/step - accuracy: 0.1047 - loss: 2.4882 - val_accuracy: 0.1072 - val_loss: 2.3076\n",
            "Epoch 8/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 544ms/step - accuracy: 0.0999 - loss: 2.4607 - val_accuracy: 0.1030 - val_loss: 2.3098\n",
            "Epoch 9/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 555ms/step - accuracy: 0.1007 - loss: 2.4274 - val_accuracy: 0.1137 - val_loss: 2.3027\n",
            "Epoch 10/10\n",
            "\u001b[1m561/561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 644ms/step - accuracy: 0.1007 - loss: 2.4134 - val_accuracy: 0.1099 - val_loss: 2.3001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d0b06a89f50>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "j0AsMTvgvArz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary mapping class codes to descriptions\n",
        "class_labels = {\n",
        "    'c0': 'safe driving',\n",
        "    'c1': 'texting - right',\n",
        "    'c2': 'talking on the phone - right',\n",
        "    'c3': 'texting - left',\n",
        "    'c4': 'talking on the phone - left',\n",
        "    'c5': 'operating the radio',\n",
        "    'c6': 'drinking',\n",
        "    'c7': 'reaching behind',\n",
        "    'c8': 'hair and makeup',\n",
        "    'c9': 'talking to passenger'\n",
        "}\n",
        "\n",
        "class_codes = {v: k for k, v in train_generator.class_indices.items()}"
      ],
      "metadata": {
        "id": "Q2552KxFvXvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_for_validation(my_image,d1,d2):\n",
        "  img=image.load_img(my_image,target_size=(d1,d2))\n",
        "  x=image.img_to_array(img)\n",
        "  x=preprocess_input(x)\n",
        "  x=np.expand_dims(x,axis=0)\n",
        "  return x"
      ],
      "metadata": {
        "id": "7cfGjleLu36g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(my_image,model,d1,d2):\n",
        "  prediction = model.predict(process_image_for_validation(my_image,d1,d2))\n",
        "  predicted_class = np.argmax(prediction)\n",
        "  # class_codes = {v: k for k, v in train_generator.class_indices.items()}\n",
        "  class_code = class_codes[predicted_class]\n",
        "  class_label = class_labels[class_code]\n",
        "  print(f\"Image Path: {my_image}\")\n",
        "  print(f\"predicted_class: {predicted_class}\")\n",
        "  print(f\"Predicted Class: \\\"{class_code} : {class_label}\\\" with Probability: {prediction[0, predicted_class]:.2f}\")"
      ],
      "metadata": {
        "id": "iyDuXNNggvoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "images = ['/content/distracted_driver_data/imgs/test/img_1.jpg',\n",
        "          '/content/distracted_driver_data/imgs/test/img_10.jpg',\n",
        "          '/content/distracted_driver_data/imgs/test/img_100.jpg',\n",
        "          '/content/distracted_driver_data/imgs/test/img_1000.jpg',\n",
        "          '/content/distracted_driver_data/imgs/test/img_100000.jpg'\n",
        "          ]"
      ],
      "metadata": {
        "id": "qjrWT9iHs1Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for im in images:\n",
        "  predict_image(im,model,*IMG_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBBQ34dZutuA",
        "outputId": "7f43f9a8-afb7-4e44-f288-53df73d7c488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Image Path: /content/distracted_driver_data/imgs/test/img_1.jpg\n",
            "predicted_class: 5\n",
            "Predicted Class: \"c5 : operating the radio\" with Probability: 0.11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Image Path: /content/distracted_driver_data/imgs/test/img_10.jpg\n",
            "predicted_class: 0\n",
            "Predicted Class: \"c0 : safe driving\" with Probability: 0.12\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Image Path: /content/distracted_driver_data/imgs/test/img_100.jpg\n",
            "predicted_class: 0\n",
            "Predicted Class: \"c0 : safe driving\" with Probability: 0.11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "Image Path: /content/distracted_driver_data/imgs/test/img_1000.jpg\n",
            "predicted_class: 0\n",
            "Predicted Class: \"c0 : safe driving\" with Probability: 0.12\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Image Path: /content/distracted_driver_data/imgs/test/img_100000.jpg\n",
            "predicted_class: 4\n",
            "Predicted Class: \"c4 : talking on the phone - left\" with Probability: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J24YGvJuuxzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}